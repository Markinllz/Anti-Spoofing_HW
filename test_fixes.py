"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤ –∫–æ–¥–µ.
"""
import warnings
import hydra
import torch
from hydra.utils import instantiate
from omegaconf import OmegaConf

from src.datasets.data_utils import get_dataloaders
from src.utils.init_utils import set_random_seed, setup_saving_and_logging

warnings.filterwarnings("ignore", category=UserWarning)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
from src.trainer.trainer_fixed import Trainer

@hydra.main(version_base=None, config_path="src/configs", config_name="baseline")
def main(config):
    """
    –¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:
    1. LR scheduler –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ü–µ —ç–ø–æ—Ö–∏
    2. Gradient clipping –æ—Ç–∫–ª—é—á–µ–Ω
    3. –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ NaN/Inf
    4. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    """
    print("üîß –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è...")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
    set_random_seed(config.trainer.seed)
    
    # –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è —Ç–µ—Å—Ç–∞
    OmegaConf.set_struct(config, False)
    config.trainer.n_epochs = 3  # –¢–æ–ª—å–∫–æ 3 —ç–ø–æ—Ö–∏ –¥–ª—è —Ç–µ—Å—Ç–∞
    config.trainer.log_step = 10  # –ß–∞—â–µ –ª–æ–≥–∏—Ä—É–µ–º
    config.optimizer.lr = 0.001  # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π LR
    config.writer.run_name = "fixes-test"
    OmegaConf.set_struct(config, True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    project_config = OmegaConf.to_container(config)
    logger = setup_saving_and_logging(config)
    writer = instantiate(config.writer, logger, project_config)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
    dataloaders, batch_transforms = get_dataloaders(config, device, debug_mode=False)
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = instantiate(config.model).to(device)
    print(f"üî¢ –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in model.parameters()):,}")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    loss_function = instantiate(config.loss_function).to(device)
    metrics = instantiate(config.metrics)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    trainable_params = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = instantiate(config.optimizer, params=trainable_params)
    lr_scheduler = instantiate(config.lr_scheduler, optimizer=optimizer)
    
    print(f"üìâ –ù–∞—á–∞–ª—å–Ω—ã–π LR: {lr_scheduler.get_last_lr()[0]}")
    print(f"üìÖ LR scheduler: —É–º–µ–Ω—å—à–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ {config.lr_scheduler.step_size} —ç–ø–æ—Ö –Ω–∞ {config.lr_scheduler.gamma}")
    
    # –°–æ–∑–¥–∞–µ–º –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(
        model=model,
        criterion=loss_function,
        metrics=metrics,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler,
        config=config,
        device=device,
        dataloaders=dataloaders,
        epoch_len=None,
        logger=logger,
        writer=writer,
        batch_transforms=batch_transforms,
        skip_oom=True,
    )
    
    print("\nüöÄ –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏...")
    print("üîß –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
    print("   ‚úÖ LR scheduler –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω –≤ –∫–æ–Ω–µ—Ü —ç–ø–æ—Ö–∏")
    print("   ‚úÖ Gradient clipping –æ—Ç–∫–ª—é—á–µ–Ω")
    print("   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ NaN/Inf")
    print("   ‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
    print("-" * 50)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–¥–∏–Ω –±–∞—Ç—á –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    print("\nüß™ –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞:")
    train_dataloader = dataloaders["train"]
    batch = next(iter(train_dataloader))
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
    print(f"üìä –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {batch['data_object'].shape}")
    print(f"üìä –ú–µ—Ç–∫–∏: {batch['labels'].shape}, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ: {torch.unique(batch['labels'])}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º forward pass
    model.eval()
    with torch.no_grad():
        batch = trainer.move_batch_to_device(batch)
        batch = trainer.transform_batch(batch)
        outputs = model(**batch)
        loss_dict = loss_function(**{**batch, **outputs})
        
        print(f"üìä Logits shape: {outputs['logits'].shape}")
        print(f"üìä Loss: {loss_dict['loss'].item():.6f}")
        print(f"üìä Logits range: [{outputs['logits'].min().item():.4f}, {outputs['logits'].max().item():.4f}]")
    
    print("\n‚úÖ –¢–µ—Å—Ç –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ!")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    trainer.train()
    
    print("\n‚úÖ –¢–µ—Å—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main() 