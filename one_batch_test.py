import warnings
import hydra
import torch
from hydra.utils import instantiate
from omegaconf import OmegaConf

from src.datasets.data_utils import get_dataloaders
from src.trainer import Trainer
from src.utils.init_utils import set_random_seed, setup_saving_and_logging

warnings.filterwarnings("ignore", category=UserWarning)

@hydra.main(version_base=None, config_path="src/configs", config_name="baseline")
def main(config):
    """
    One Batch Test script. Tests if the model can overfit on a small batch.
    This is useful for debugging the training pipeline.
    """
    print("üîç –ó–∞–ø—É—Å–∫ One Batch Test...")
    print("üìä –û–∂–∏–¥–∞–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ")
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    set_random_seed(config.trainer.seed)
    
    # –î–æ–±–∞–≤–ª—è–µ–º debug_mode –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    OmegaConf.set_struct(config, False)
    config.debug_mode = True
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
    config.trainer.n_epochs = 100
    # –ß–∞—â–µ –ª–æ–≥–∏—Ä—É–µ–º
    config.trainer.log_step = 1
    # –û—Ç–∫–ª—é—á–∞–µ–º early stopping
    config.trainer.early_stop = 200
    OmegaConf.set_struct(config, True)
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    project_config = OmegaConf.to_container(config)
    logger = setup_saving_and_logging(config)
    writer = instantiate(config.writer, logger, project_config)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    if config.trainer.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = config.trainer.device
    
    print(f"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    # –°–æ–∑–¥–∞–µ–º –º–∏–Ω–∏-–¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã (debug_mode=True)
    dataloaders, batch_transforms = get_dataloaders(config, device, debug_mode=True)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–Ω–∏ –∏ —Ç–µ –∂–µ –¥–∞–Ω–Ω—ã–µ
    print("\nüìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ One Batch Test:")
    train_batch = next(iter(dataloaders["train"]))
    val_batch = next(iter(dataloaders["val"]))
    
    print(f"üìù Train batch shape: {train_batch['data_object'].shape}")
    print(f"üìù Val batch shape: {val_batch['data_object'].shape}")
    print(f"üìù Train labels: {train_batch['labels']}")
    print(f"üìù Val labels: {val_batch['labels']}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ª–∏ –¥–∞–Ω–Ω—ã–µ
    data_identical = torch.equal(train_batch['data_object'], val_batch['data_object'])
    labels_identical = torch.equal(train_batch['labels'], val_batch['labels'])
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {data_identical}")
    print(f"‚úÖ –ú–µ—Ç–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã: {labels_identical}")
    
    if not (data_identical and labels_identical):
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –î–∞–Ω–Ω—ã–µ train –∏ val –Ω–µ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã!")
    else:
        print("üéØ –û—Ç–ª–∏—á–Ω–æ: Train –∏ Val –∏—Å–ø–æ–ª—å–∑—É—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ")
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = instantiate(config.model).to(device)
    
    # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"üî¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
    print(f"üéØ –û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {trainable_params:,}")
    
    # –°–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –∏ –º–µ—Ç—Ä–∏–∫–∏
    loss_function = instantiate(config.loss_function).to(device)
    metrics = instantiate(config.metrics)
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
    trainable_params_iter = filter(lambda p: p.requires_grad, model.parameters())
    optimizer = instantiate(config.optimizer, params=trainable_params_iter)
    lr_scheduler = instantiate(config.lr_scheduler, optimizer=optimizer)
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(
        model=model,
        criterion=loss_function,
        metrics=metrics,
        optimizer=optimizer,
        lr_scheduler=lr_scheduler,
        config=config,
        device=device,
        dataloaders=dataloaders,
        epoch_len=None,
        logger=logger,
        writer=writer,
        batch_transforms=batch_transforms,
        skip_oom=config.trainer.get("skip_oom", True),
    )
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º One Batch Test...")
    print("üìà –û–∂–∏–¥–∞–µ–º–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ:")
    print("   - Loss –¥–æ–ª–∂–µ–Ω –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ —É–º–µ–Ω—å—à–∞—Ç—å—Å—è")
    print("   - –ß–µ—Ä–µ–∑ 50-100 —ç–ø–æ—Ö loss –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å < 0.01")
    print("   - Accuracy –¥–æ–ª–∂–Ω–∞ —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ 100%")
    print("   - EER –¥–æ–ª–∂–Ω–∞ —Å—Ç—Ä–µ–º–∏—Ç—å—Å—è –∫ 0%")
    print("-" * 50)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    trainer.train()
    
    print("‚úÖ One Batch Test –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    main() 