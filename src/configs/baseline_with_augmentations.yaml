
defaults:
  - baseline
  - _self_


transforms:
  augmentations:
    anti_spoofing:
      _target_: src.transforms.augmentations.get_anti_spoofing_augmentations
      p: 0.1

trainer:
  epochs: 50
  save_period: 5
  log_step: 50

optimizer:
  _target_: torch.optim.Adam
  lr: 0.0003
  weight_decay: 0.002

lr_scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 5 
  gamma: 0.7 

loss_function:
  _target_: src.loss.crossentropy.CrossEntropyLoss
  label_smoothing: 0.2

monitoring:
  mnt_metric: dev_eer
  mnt_mode: min
  mnt_best: inf
  early_stopping: 5