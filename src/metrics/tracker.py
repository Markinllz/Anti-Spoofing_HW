# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è tracker.py
import pandas as pd
import numpy as np
from collections import defaultdict


class MetricTracker:
    """
    Class to aggregate metrics from many batches.
    """

    def __init__(self, *keys, writer=None):
        """
        Args:
            *keys (list[str]): list (as positional arguments) of metric
                names (may include the names of losses)
            writer (WandBWriter | CometMLWriter | None): experiment tracker.
                Not used in this code version. Can be used to log metrics
                from each batch.
        """
        self.writer = writer
        self._data = pd.DataFrame(index=keys, columns=["total", "counts", "average"])
        self.reset()
        
        
        self._eer_scores = []
        self._eer_labels = []

    def reset(self):
        """
        Reset all metrics after epoch end.
        """
        for col in self._data.columns:
            self._data[col].values[:] = 0
        
        # –°–±—Ä–æ—Å EER –¥–∞–Ω–Ω—ã—Ö
        self._eer_scores = []
        self._eer_labels = []
        print(f"üîÑ MetricTracker —Å–±—Ä–æ—à–µ–Ω. EER lists –æ—á–∏—â–µ–Ω—ã.")

    def update(self, key, value, n=1):
        

        self._data.loc[key, "total"] += value * n
        self._data.loc[key, "counts"] += n
        self._data.loc[key, "average"] = self._data.total[key] / self._data.counts[key]

    def update_eer(self, scores, labels):
        """
        Update EER scores and labels.
        
        Args:
            scores (torch.Tensor): prediction scores
            labels (torch.Tensor): ground truth labels
        """
        if scores.numel() == 0 or labels.numel() == 0:
            print(f"‚ö†Ô∏è update_eer: –ø–æ–ª—É—á–µ–Ω—ã –ø—É—Å—Ç—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã")
            return
            
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy
        scores_np = scores.detach().cpu().numpy().flatten()
        labels_np = labels.detach().cpu().numpy().flatten()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
        if len(scores_np) != len(labels_np):
            print(f"‚ö†Ô∏è update_eer: —Ä–∞–∑–º–µ—Ä—ã –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç scores={len(scores_np)}, labels={len(labels_np)}")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        if np.any(np.isnan(scores_np)) or np.any(np.isinf(scores_np)):
            print(f"‚ö†Ô∏è update_eer: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ scores (nan/inf)")
            return
            
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        self._eer_scores.extend(scores_np)
        self._eer_labels.extend(labels_np)

    def compute_eer(self):
        """
        Compute Equal Error Rate from accumulated scores and labels.
        """
        if not self._eer_scores or len(self._eer_scores) == 0:
            print(f"‚ö†Ô∏è compute_eer: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è EER")
            return 0.0
        
        scores = np.array(self._eer_scores)
        labels = np.array(self._eer_labels)
        
        print(f"üßÆ –í—ã—á–∏—Å–ª—è–µ–º EER: {len(scores)} —Å–µ–º–ø–ª–æ–≤")
        print(f"    –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ labels: {np.unique(labels)}")
        print(f"    Scores range: {scores.min():.4f} - {scores.max():.4f}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞
        unique_labels = np.unique(labels)
        if len(unique_labels) < 2:
            print(f"‚ö†Ô∏è compute_eer: —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å {unique_labels}")
            return 0.0
        
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
        thresholds = np.unique(scores)
        if len(thresholds) < 2:
            print(f"‚ö†Ô∏è compute_eer: –≤—Å–µ scores –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ")
            return 0.5  # Random baseline
        
        # –í—ã—á–∏—Å–ª—è–µ–º FAR –∏ FRR –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        far_values = []
        frr_values = []
        
        for threshold in thresholds:
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: 1 –µ—Å–ª–∏ score >= threshold, –∏–Ω–∞—á–µ 0
            predictions = (scores >= threshold).astype(int)
            
            # –í—ã—á–∏—Å–ª—è–µ–º confusion matrix
            tp = np.sum((predictions == 1) & (labels == 1))
            tn = np.sum((predictions == 0) & (labels == 0))
            fp = np.sum((predictions == 1) & (labels == 0))
            fn = np.sum((predictions == 0) & (labels == 1))
            
            # –í—ã—á–∏—Å–ª—è–µ–º FAR –∏ FRR
            far = fp / (fp + tn) if (fp + tn) > 0 else 0
            frr = fn / (fn + tp) if (fn + tp) > 0 else 0
            
            far_values.append(far)
            frr_values.append(frr)
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫—É, –≥–¥–µ FAR ‚âà FRR
        far_values = np.array(far_values)
        frr_values = np.array(frr_values)
        
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å, –≥–¥–µ —Ä–∞–∑–Ω–æ—Å—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω–∞
        diff = np.abs(far_values - frr_values)
        min_idx = np.argmin(diff)
        
        # EER - —ç—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ FAR –∏ FRR –≤ —ç—Ç–æ–π —Ç–æ—á–∫–µ
        eer = (far_values[min_idx] + frr_values[min_idx]) / 2
        
        print(f"‚úÖ EER –≤—ã—á–∏—Å–ª–µ–Ω: {eer:.6f} (FAR={far_values[min_idx]:.6f}, FRR={frr_values[min_idx]:.6f})")
        
        return float(eer)

    def avg(self, key):
        """
        Return average value for a given metric.

        Args:
            key (str): metric name.
        Returns:
            average_value (float): average value for the metric.
        """
        if key == "eer":
            return self.compute_eer()
        return self._data.average[key]

    def result(self):
        """
        Return average value of each metric.

        Returns:
            average_metrics (dict): dict, containing average metrics
                for each metric name.
        """
        result = dict(self._data.average)
        # –í—Å–µ–≥–¥–∞ –≤–∫–ª—é—á–∞–µ–º EER –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result['eer'] = self.compute_eer()
        return result

    def keys(self):
        """
        Return all metric names defined in the MetricTracker.

        Returns:
            metric_keys (Index): all metric names in the table.
        """
        keys = list(self._data.total.keys())
        if self._eer_scores:
            keys.append('eer')
        return keys