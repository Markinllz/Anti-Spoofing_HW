diff --git a/checkpoints/inference-test/config.yaml b/checkpoints/inference-test/config.yaml
index 67acd14..61adced 100644
--- a/checkpoints/inference-test/config.yaml
+++ b/checkpoints/inference-test/config.yaml
@@ -8,16 +8,14 @@ model:
   dropout_p: 0.3
 optimizer:
   _target_: torch.optim.Adam
-  lr: 1.0e-05
+  lr: 0.001
   weight_decay: 0.0001
 lr_scheduler:
   _target_: torch.optim.lr_scheduler.StepLR
-  step_size: 10
-  gamma: 0.1
+  step_size: 20
+  gamma: 0.5
 loss_function:
-  _target_: src.loss.Asoftmax.AsoftMax
-  margin: 2
-  scale: 15
+  _target_: src.loss.crossentropy.CrossEntropyLoss
 metrics:
   train:
   - _target_: src.metrics.eer.EERMetric
@@ -56,7 +54,10 @@ transforms:
   instance_transforms:
     data_object: ${transforms.stft}
   stft:
-    _target_: src.transforms.stft.AudioFrontend
+    _target_: src.transforms.stft.STFTTransform
+    n_fft: 1024
+    hop_length: 512
+    win_length: 1024
   batch_transforms:
     train:
       data_object:
@@ -80,7 +81,7 @@ writer:
   _target_: src.logger.cometml.CometMLWriter
   project_name: anti-spoofing
   workspace: null
-  run_id: xzqp1djl
+  run_id: 8gzucarc
   run_name: inference-test
   mode: online
   loss_names:
@@ -95,7 +96,7 @@ trainer:
   save_period: 10
   val_period: 1
   skip_oom: true
-  max_grad_norm: 1.0
+  max_grad_norm: 0.5
   log_step: 50
   save_dir: checkpoints
   device_tensors:
diff --git a/checkpoints/inference-test/git_commit.txt b/checkpoints/inference-test/git_commit.txt
index 1bf8e23..24ff1b6 100644
--- a/checkpoints/inference-test/git_commit.txt
+++ b/checkpoints/inference-test/git_commit.txt
@@ -1 +1 @@
-f24def3dab4de63a438094983749d95d2202c3cf
+2234c3a2bbb1302441afc29a25fec84e32271ba1
diff --git a/checkpoints/inference-test/git_diff.patch b/checkpoints/inference-test/git_diff.patch
index 1bf2ab0..e69de29 100644
--- a/checkpoints/inference-test/git_diff.patch
+++ b/checkpoints/inference-test/git_diff.patch
@@ -1,185 +0,0 @@
-diff --git a/src/configs/baseline.yaml b/src/configs/baseline.yaml
-index 6c8ba46..1649280 100644
---- a/src/configs/baseline.yaml
-+++ b/src/configs/baseline.yaml
-@@ -8,7 +8,7 @@ defaults:
-   - datasets: asvspoof2019
-   - dataloader: default
-   - transforms: default
--  - writer: wandb
-+  - writer: cometml
-   - trainer: default
- 
- data_path: ${oc.env:DATA_PATH,data}
-diff --git a/src/configs/dataloader/default.yaml b/src/configs/dataloader/default.yaml
-index 86758af..902e80f 100644
---- a/src/configs/dataloader/default.yaml
-+++ b/src/configs/dataloader/default.yaml
-@@ -1,4 +1,4 @@
- _target_: torch.utils.data.DataLoader
--batch_size: 32
-+batch_size: 16
- num_workers: 4
- pin_memory: true 
-\ No newline at end of file
-diff --git a/src/configs/loss_function/asoftmax.yaml b/src/configs/loss_function/asoftmax.yaml
-index fe087a7..a96439b 100644
---- a/src/configs/loss_function/asoftmax.yaml
-+++ b/src/configs/loss_function/asoftmax.yaml
-@@ -1,3 +1,3 @@
- _target_: src.loss.Asoftmax.AsoftMax
--margin: 4
--scale: 30 
-\ No newline at end of file
-+margin: 2
-+scale: 15 
-\ No newline at end of file
-diff --git a/src/configs/optimizer/adam.yaml b/src/configs/optimizer/adam.yaml
-index 9af2fd8..3b50a71 100644
---- a/src/configs/optimizer/adam.yaml
-+++ b/src/configs/optimizer/adam.yaml
-@@ -1,3 +1,3 @@
- _target_: torch.optim.Adam
--lr: 0.0001
-+lr: 0.00001
- weight_decay: 0.0001 
-\ No newline at end of file
-diff --git a/src/loss/Asoftmax.py b/src/loss/Asoftmax.py
-index 25a74e3..6c4ee07 100644
---- a/src/loss/Asoftmax.py
-+++ b/src/loss/Asoftmax.py
-@@ -20,26 +20,29 @@ class AsoftMax(nn.Module):
-         Returns:
-             losses (dict): dictionary loss
-         """
--       
-+        
-+        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ logits
-         logits_norm = F.normalize(logits, p=2, dim=1)
--        prev_cos = torch.clamp(logits_norm, -1.0 + 1e-6, 1.0 - 1e-6)
-         
--        angle = torch.acos(prev_cos)
--        cos_m = prev_cos.clone()
-+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÐºÐ¾ÑÐ¸Ð½ÑƒÑ ÑƒÐ³Ð»Ð°
-+        cos_theta = torch.clamp(logits_norm, -1.0 + 1e-6, 1.0 - 1e-6)
-         
-+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ ÑƒÐ³Ð¾Ð»
-+        theta = torch.acos(cos_theta)
-         
--        mask = torch.zeros_like(prev_cos)
-+        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¼Ð°ÑÐºÑƒ Ð´Ð»Ñ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ°
-+        mask = torch.zeros_like(cos_theta)
-         mask.scatter_(1, labels.unsqueeze(1), 1)
-         
--       
--        cos_m = torch.where(mask == 1, 
--                                 torch.cos(self.margin * angle), 
--                                 prev_cos)
-+        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ margin Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ðº Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¼Ñƒ ÐºÐ»Ð°ÑÑÑƒ
-+        cos_theta_m = torch.where(mask == 1, 
-+                                 torch.cos(self.margin * theta), 
-+                                 cos_theta)
-         
--       
--        cos_m = cos_m * self.scale
-+        # ÐœÐ°ÑÑˆÑ‚Ð°Ð±Ð¸Ñ€ÑƒÐµÐ¼
-+        cos_theta_m = cos_theta_m * self.scale
-         
--   
--        loss = F.cross_entropy(cos_m, labels)
-+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ loss
-+        loss = F.cross_entropy(cos_theta_m, labels)
-         
-         return {"loss": loss}
-\ No newline at end of file
-diff --git a/src/metrics/eer.py b/src/metrics/eer.py
-index 61d466e..7e34321 100644
---- a/src/metrics/eer.py
-+++ b/src/metrics/eer.py
-@@ -1,4 +1,5 @@
- import numpy as np
-+import torch
- from abc import abstractmethod
- 
- class BaseMetric:
-@@ -17,8 +18,8 @@ class EERMetric(BaseMetric):
-     """
-     Equal Error Rate (EER) metric.
-     ÐžÐ¶Ð¸Ð´Ð°ÐµÑ‚ Ð² batch Ð´Ð²Ð° Ð¿Ð¾Ð»Ñ:
--        - 'scores': numpy array Ð¸Ð»Ð¸ torch tensor Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¼Ð¸ ÑÐºÐ¾Ñ€Ð¸Ð½Ð³Ð°Ð¼Ð¸
--        - 'labels': numpy array Ð¸Ð»Ð¸ torch tensor Ñ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸ (1 â€” bona fide, 0 â€” spoof)
-+        - 'logits': torch tensor Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¼Ð¸ logits
-+        - 'labels': torch tensor Ñ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸ (1 â€” bona fide, 0 â€” spoof)
-     """
- 
-     def __init__(self, name="eer"):
-@@ -50,6 +51,9 @@ class EERMetric(BaseMetric):
-         bona_scores = scores[labels == 1]
-         spoof_scores = scores[labels == 0]
- 
-+        if len(bona_scores) == 0 or len(spoof_scores) == 0:
-+            return 0.0
-+
-         eer, _ = self.compute_eer(bona_scores, spoof_scores)
-         return eer
- 
-diff --git a/src/model/model.py b/src/model/model.py
-index 6d0bf94..28a4ce4 100644
---- a/src/model/model.py
-+++ b/src/model/model.py
-@@ -66,13 +66,29 @@ class LCNN(nn.Module):
- 
-         self.MaxPool28 = nn.MaxPool2d(kernel_size=2, stride=2)
- 
--
-         self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
-         self.fc29 = nn.Linear(32, 160)
-         self.dropout29 = nn.Dropout(dropout_p)
-         self.mfm30 = mfm_block(160)
-         self.BatchNorm31 = nn.BatchNorm1d(80)
-         self.fc32 = nn.Linear(80, num_classes)
-+        
-+        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð²
-+        self._initialize_weights()
-+
-+    def _initialize_weights(self):
-+        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
-+        for m in self.modules():
-+            if isinstance(m, nn.Conv2d):
-+                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-+                if m.bias is not None:
-+                    nn.init.constant_(m.bias, 0)
-+            elif isinstance(m, nn.BatchNorm2d):
-+                nn.init.constant_(m.weight, 1)
-+                nn.init.constant_(m.bias, 0)
-+            elif isinstance(m, nn.Linear):
-+                nn.init.normal_(m.weight, 0, 0.01)
-+                nn.init.constant_(m.bias, 0)
- 
-     def forward(self, data_object, **kwargs):
-         x = data_object
-diff --git a/src/trainer/trainer.py b/src/trainer/trainer.py
-index 1b806fb..3313fea 100644
---- a/src/trainer/trainer.py
-+++ b/src/trainer/trainer.py
-@@ -47,17 +47,17 @@ class Trainer(BaseTrainer):
-             if self.lr_scheduler is not None:
-                 self.lr_scheduler.step()
- 
--        
-+        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ loss Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
-         for loss_name in self.config.writer.loss_names:
-             metrics.update(loss_name, batch[loss_name].item())
- 
--      
-+        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ EER Ð¼ÐµÑ‚Ñ€Ð¸ÐºÑƒ
-         if "logits" in batch:
-             scores = torch.softmax(batch["logits"], dim=1)[:, 1]
-             labels = batch["labels"]
-             metrics.update_eer(scores, labels)
- 
--       
-+        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐ¸
-         for met in metric_funcs:
-             if met.name != "eer":
-                 try:
diff --git a/requirements.txt b/requirements.txt
index 8aab454..1dc9376 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,37 +1,24 @@
-# ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ PyTorch Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸
 torch==2.2.0
 torchvision==0.17.0
 torchaudio==2.2.0
 torchmetrics==1.7.4
-
-# ÐÐ°ÑƒÑ‡Ð½Ñ‹Ðµ Ð²Ñ‹Ñ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ñ Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…
 numpy==1.26.4
 pandas==2.3.1
 matplotlib==3.9.4
 scipy
-
-# ÐÑƒÐ´Ð¸Ð¾ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ°
 soundfile==0.13.1
 librosa
-
-# Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ñ‹
 wandb==0.21.0
 comet-ml==3.50.0
 hydra-core==1.3.2
 omegaconf==2.3.0
-
-# Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹
 tqdm==4.67.1
 psutil==7.0.0
 requests==2.32.4
 pyyaml==6.0.2
-
-# Ð Ð°Ð·Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð´Ð°
 black
 isort
 pre-commit
 flake8
-
-# Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð´Ð»Ñ ML
 scikit-learn
 seaborn
\ No newline at end of file
diff --git a/src/configs/dataloader/default.yaml b/src/configs/dataloader/default.yaml
index 80fbc65..902e80f 100644
--- a/src/configs/dataloader/default.yaml
+++ b/src/configs/dataloader/default.yaml
@@ -1,4 +1,4 @@
 _target_: torch.utils.data.DataLoader
-batch_size: 8
+batch_size: 16
 num_workers: 4
 pin_memory: true 
\ No newline at end of file
diff --git a/src/configs/lr_scheduler/step.yaml b/src/configs/lr_scheduler/step.yaml
index bd12a00..3c208ac 100644
--- a/src/configs/lr_scheduler/step.yaml
+++ b/src/configs/lr_scheduler/step.yaml
@@ -1,3 +1,3 @@
 _target_: torch.optim.lr_scheduler.StepLR
-step_size: 10
-gamma: 0.1 
\ No newline at end of file
+step_size: 20
+gamma: 0.5 
\ No newline at end of file
diff --git a/src/configs/optimizer/adam.yaml b/src/configs/optimizer/adam.yaml
index 9af2fd8..1310b87 100644
--- a/src/configs/optimizer/adam.yaml
+++ b/src/configs/optimizer/adam.yaml
@@ -1,3 +1,3 @@
 _target_: torch.optim.Adam
-lr: 0.0001
+lr: 0.001
 weight_decay: 0.0001 
\ No newline at end of file
diff --git a/src/configs/transforms/default.yaml b/src/configs/transforms/default.yaml
index 4aea72d..32f1775 100644
--- a/src/configs/transforms/default.yaml
+++ b/src/configs/transforms/default.yaml
@@ -2,7 +2,10 @@ instance_transforms:
   data_object: ${transforms.stft}
 
 stft:
-  _target_: src.transforms.stft.AudioFrontend
+  _target_: src.transforms.stft.STFTTransform
+  n_fft: 1024
+  hop_length: 512
+  win_length: 1024
 
 batch_transforms:
   train:
diff --git a/src/datasets/base_dataset.py b/src/datasets/base_dataset.py
index bf51c70..e75525d 100644
--- a/src/datasets/base_dataset.py
+++ b/src/datasets/base_dataset.py
@@ -1,6 +1,6 @@
 import logging
 import random
-from typing import List
+from typing import List, Dict, Any, Optional
 
 import torch
 import torchaudio
@@ -11,66 +11,65 @@ logger = logging.getLogger(__name__)
 
 class BaseDataset(Dataset):
     """
-    Base class for the datasets.
-
-    Given a proper index (list[dict]), allows to process different datasets
-    for the same task in the identical manner. Therefore, to work with
-    several datasets, the user only have to define index in a nested class.
+    Base class for all datasets.
     """
 
     def __init__(
-        self, index, limit=None, shuffle_index=False, instance_transforms=None
+        self,
+        index: List[Dict[str, Any]],
+        instance_transforms: Optional[Dict[str, Any]] = None,
+        *args,
+        **kwargs,
     ):
         """
         Args:
-            index (list[dict]): list, containing dict for each element of
-                the dataset. The dict has required metadata information,
-                such as label and object path.
-            limit (int | None): if not None, limit the total number of elements
-                in the dataset to 'limit' elements.
-            shuffle_index (bool): if True, shuffle the index. Uses python
-                random package with seed 42.
-            instance_transforms (dict[Callable] | None): transforms that
-                should be applied on the instance. Depend on the
-                tensor name.
-        """
-        self._assert_index_is_valid(index)
-
-        index = self._shuffle_and_limit_index(index, limit, shuffle_index)
-        self._index: List[dict] = index
-
+            index (List[Dict[str, Any]]): list of dictionaries, each containing
+                the data for one sample.
+            instance_transforms (Optional[Dict[str, Any]]): transforms to apply
+                to instances. Depend on the tensor name.
+        """
+        self.index = index
         self.instance_transforms = instance_transforms
 
-    def __getitem__(self, ind):
-        """
-        Get element from the index, preprocess it, and combine it
-        into a dict.
+    def __len__(self):
+        return len(self.index)
 
-        Notice that the choice of key names is defined by the template user.
-        However, they should be consistent across dataset getitem, collate_fn,
-        loss_function forward method, and model forward method.
+    def __getitem__(self, idx):
+        """
+        Get item by index.
 
         Args:
-            ind (int): index in the self.index list.
+            idx (int): index of the item.
+
         Returns:
-            instance_data (dict): dict, containing instance
-                (a single dataset element).
+            dict: item data.
         """
-        data_dict = self._index[ind]
-        data_path = data_dict["path"]
-        data_object = self.load_object(data_path)
-        data_label = data_dict["label"]
+        item = self.index[idx]
+        
+        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ instance transforms
+        if self.instance_transforms is not None:
+            item = self._apply_instance_transforms(item)
+        
+        return item
 
-        instance_data = {"data_object": data_object, "labels": data_label}
-        instance_data = self.preprocess_data(instance_data)
+    def _apply_instance_transforms(self, item: Dict[str, Any]) -> Dict[str, Any]:
+        """
+        Apply instance transforms to the item.
 
-        return instance_data
+        Args:
+            item (Dict[str, Any]): item data.
 
-    def __len__(self):
-        """
-        Get length of the dataset (length of the index).
+        Returns:
+            Dict[str, Any]: transformed item data.
         """
-        return len(self._index)
+        for transform_name, transform in self.instance_transforms.items():
+            if transform_name in item:
+                try:
+                    item[transform_name] = transform(item[transform_name])
+                except Exception as e:
+                    raise
+        
+        return item
 
     def load_object(self, path):
         """
@@ -126,74 +125,63 @@ class BaseDataset(Dataset):
         the __init__ before shuffling and limiting.
 
         Args:
-            index (list[dict]): list, containing dict for each element of
-                the dataset. The dict has required metadata information,
-                such as label and object path.
+            index (list): list of records to filter.
+
         Returns:
-            index (list[dict]): list, containing dict for each element of
-                the dataset that satisfied the condition. The dict has
-                required metadata information, such as label and object path.
+            list: filtered list of records.
         """
-        # Filter logic
-        pass
+        return index
 
     @staticmethod
     def _assert_index_is_valid(index):
         """
-        Check the structure of the index and ensure it satisfies the desired
-        conditions.
+        Assert that the index is valid.
 
         Args:
-            index (list[dict]): list, containing dict for each element of
-                the dataset. The dict has required metadata information,
-                such as label and object path.
-        """
-        for entry in index:
-            assert "path" in entry, (
-                "Each dataset item should include field 'path'" " - path to audio file."
-            )
-            assert "label" in entry, (
-                "Each dataset item should include field 'label'"
-                " - object ground-truth label."
-            )
+            index (list): list of records to validate.
+        """
+        assert isinstance(index, list), "Index should be a list"
+        assert len(index) > 0, "Index should not be empty"
+        for record in index:
+            assert isinstance(record, dict), "Each record should be a dict"
+            assert "path" in record, "Each record should have a 'path' field"
+            assert "label" in record, "Each record should have a 'label' field"
 
     @staticmethod
     def _sort_index(index):
         """
-        Sort index via some rules.
+        Sort the index by some criterion.
 
         This is not used in the example. The method should be called in
-        the __init__ before shuffling and limiting and after filtering.
+        the __init__ before shuffling and limiting.
 
         Args:
-            index (list[dict]): list, containing dict for each element of
-                the dataset. The dict has required metadata information,
-                such as label and object path.
+            index (list): list of records to sort.
+
         Returns:
-            index (list[dict]): sorted list, containing dict for each element
-                of the dataset. The dict has required metadata information,
-                such as label and object path.
+            list: sorted list of records.
         """
-        return sorted(index, key=lambda x: x["KEY_FOR_SORTING"])
+        return index
 
     @staticmethod
     def _shuffle_and_limit_index(index, limit, shuffle_index):
         """
-        Shuffle elements in index and limit the total number of elements.
+        Shuffle and limit the index.
+
+        This is not used in the example. The method should be called in
+        the __init__ before shuffling and limiting.
 
         Args:
-            index (list[dict]): list, containing dict for each element of
-                the dataset. The dict has required metadata information,
-                such as label and object path.
-            limit (int | None): if not None, limit the total number of elements
-                in the dataset to 'limit' elements.
-            shuffle_index (bool): if True, shuffle the index. Uses python
-                random package with seed 42.
+            index (list): list of records to shuffle and limit.
+            limit (int): maximum number of records to keep.
+            shuffle_index (bool): whether to shuffle the index.
+
+        Returns:
+            list: shuffled and limited list of records.
         """
         if shuffle_index:
             random.seed(42)
             random.shuffle(index)
-
         if limit is not None:
             index = index[:limit]
         return index
\ No newline at end of file
diff --git a/src/datasets/collate.py b/src/datasets/collate.py
index 4c2c81e..6cb3b10 100644
--- a/src/datasets/collate.py
+++ b/src/datasets/collate.py
@@ -19,6 +19,15 @@ def collate_fn(dataset_items: list[dict]):
 
     # Pad audio sequences to the same length
     audio_tensors = [elem["data_object"] for elem in dataset_items]
+    
+    # Handle different audio tensor shapes
+    if len(audio_tensors) == 0:
+        # Return empty batch
+        result_batch["data_object"] = torch.empty(0)
+        result_batch["labels"] = torch.empty(0, dtype=torch.long)
+        return result_batch
+    
+    # Get max length for padding
     max_length = max(audio.shape[-1] for audio in audio_tensors)
     
     padded_audio = []
@@ -30,6 +39,6 @@ def collate_fn(dataset_items: list[dict]):
         padded_audio.append(audio)
     
     result_batch["data_object"] = torch.stack(padded_audio)
-    result_batch["labels"] = torch.tensor([elem["labels"] for elem in dataset_items])
+    result_batch["labels"] = torch.tensor([elem["labels"] for elem in dataset_items], dtype=torch.long)
 
     return result_batch
\ No newline at end of file
diff --git a/src/datasets/data_utils.py b/src/datasets/data_utils.py
index 8262ceb..63e8cce 100644
--- a/src/datasets/data_utils.py
+++ b/src/datasets/data_utils.py
@@ -36,6 +36,9 @@ def move_batch_transforms_to_device(batch_transforms, device):
             tensor name.
         device (str): device to use for batch transforms.
     """
+    if batch_transforms is None:
+        return
+        
     for transform_type in batch_transforms.keys():
         transforms = batch_transforms.get(transform_type)
         if transforms is not None:
@@ -60,6 +63,7 @@ def get_dataloaders(config, device):
     """
     # transforms or augmentations init
     batch_transforms = instantiate(config.transforms.batch_transforms)
+    
     move_batch_transforms_to_device(batch_transforms, device)
 
     # dataset partitions init
diff --git a/src/datasets/mydataset.py b/src/datasets/mydataset.py
index 02a09c3..94e7e12 100644
--- a/src/datasets/mydataset.py
+++ b/src/datasets/mydataset.py
@@ -1,5 +1,6 @@
 import numpy as np
 import torch
+import torchaudio
 from tqdm.auto import tqdm
 
 from src.datasets.base_dataset import BaseDataset
@@ -37,6 +38,50 @@ class AudioSpoofingDataset(BaseDataset):
 
         super().__init__(index, instance_transforms=instance_transforms, *args, **kwargs)
 
+    def __getitem__(self, idx):
+        """
+        Get item by index.
+
+        Args:
+            idx (int): index of the item.
+
+        Returns:
+            dict: item data with 'data_object' and 'labels' keys.
+        """
+        item = self.index[idx]
+        
+        # Load audio file
+        audio_path = item["path"]
+        label = item["label"]
+        
+        try:
+            # Load audio using torchaudio
+            waveform, sample_rate = torchaudio.load(audio_path)
+            
+            # Convert to mono if stereo
+            if waveform.shape[0] > 1:
+                waveform = torch.mean(waveform, dim=0, keepdim=True)
+            
+            # Create item with correct keys
+            item_data = {
+                "data_object": waveform,
+                "labels": label
+            }
+            
+            # Apply instance transforms
+            if self.instance_transforms is not None:
+                item_data = self._apply_instance_transforms(item_data)
+            
+            return item_data
+            
+        except Exception as e:
+            # Return zero tensor as fallback
+            fallback_waveform = torch.zeros(1, 16000)  # 1 second of silence at 16kHz
+            return {
+                "data_object": fallback_waveform,
+                "labels": label
+            }
+
     def _create_index(self, label_path, audio_path, out_path):
         """
         Args:
@@ -47,25 +92,40 @@ class AudioSpoofingDataset(BaseDataset):
         Returns:
             index (list[dict]): list of dictionaries, each with "path" and "label" fields
         """
-
         index = []
+        
+        # ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ð¾Ð±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÑ‚Ñ€Ð¾Ðº Ð² Ñ„Ð°Ð¹Ð»Ðµ
+        with open(label_path, "r") as f:
+            total_lines = sum(1 for _ in f)
+        
+        bonafide_count = 0
+        spoof_count = 0
 
         with open(label_path, "r") as f:
-            for line in f:
+            for line_num, line in enumerate(tqdm(f, total=total_lines, desc="ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÑ‚Ñ€Ð¾Ðº")):
                 parts = line.strip().split()
                 file_id = parts[1]
                 class_name = parts[-1]
                 label = 0 if class_name == "bonafide" else 1  # Fixed typo
                 path = str(Path(audio_path) / f"{file_id}.flac")
+                
+                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑÑƒÑ‰ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð°Ð¹Ð»Ð°
+                if not Path(path).exists():
+                    continue
+                
                 index.append(
                     {
                         "path" : path,
                         "label" : label
                     }
                 )
-        print("Separate to path and labels complete")
+                
+                # ÐŸÐ¾Ð´ÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ
+                if label == 0:
+                    bonafide_count += 1
+                else:
+                    spoof_count += 1
+        
         write_json(index, out_path)
 
-        print(f"Created {len(index)} entries in {out_path}")
-
         return index
diff --git a/src/loss/crossentropy.py b/src/loss/crossentropy.py
index d8ad43e..677200c 100644
--- a/src/loss/crossentropy.py
+++ b/src/loss/crossentropy.py
@@ -1,28 +1,80 @@
 import torch
-from torch import nn
-import torch.nn.functional as F
+import torch.nn as nn
+from typing import Dict, Any
+
 
 class CrossEntropyLoss(nn.Module):
     """
-    ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ CrossEntropy loss Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ
+    Cross Entropy Loss for audio anti-spoofing.
     """
 
-    def __init__(self):
-        super().__init__()
+    def __init__(self, **kwargs):
+        """
+        Args:
+            **kwargs: additional arguments
+        """
+        super(CrossEntropyLoss, self).__init__()
+        
+        print("ðŸŽ¯ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ CrossEntropyLoss...")
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
+        for key, value in kwargs.items():
+            print(f"   ðŸ“Š {key}: {value}")
+        
+        self.criterion = nn.CrossEntropyLoss()
+        print("âœ… CrossEntropyLoss Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
 
-    def forward(self, logits: torch.Tensor, labels: torch.Tensor, **kwargs):
+    def forward(self, **batch) -> Dict[str, torch.Tensor]:
         """
-        CrossEntropy loss compute
+        Compute cross entropy loss.
         
         Args:
-            logits (Tensor): model output predictions (batch_size, num_classes)
-            labels (Tensor): ground truth labels (batch_size,)
-            **kwargs: Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (Ð¸Ð³Ð½Ð¾Ñ€Ð¸Ñ€ÑƒÑŽÑ‚ÑÑ)
+            **batch: input batch containing logits and labels
+            
         Returns:
-            losses (dict): dictionary loss
+            Dict[str, torch.Tensor]: loss dictionary
         """
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸)
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ’” CrossEntropyLoss forward: Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸ {list(batch.keys())}")
+            for key, value in batch.items():
+                if isinstance(value, torch.Tensor):
+                    print(f"      {key}: shape={value.shape}, dtype={value.dtype}")
+        
+        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ logits Ð¸ labels
+        logits = batch['logits']
+        labels = batch['labels']
         
-        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹ CrossEntropy loss Ð´Ð»Ñ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
-        loss = F.cross_entropy(logits, labels)
+        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹
+        if logits.dim() == 1:
+            logits = logits.unsqueeze(0)
+        if labels.dim() == 0:
+            labels = labels.unsqueeze(0)
         
-        return {"loss": loss}
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“Š Logits: shape={logits.shape}, range=[{logits.min().item():.4f}, {logits.max().item():.4f}]")
+            print(f"   ðŸ“Š Labels: shape={labels.shape}, unique={torch.unique(labels).tolist()}")
+        
+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ Ð¿Ð¾Ñ‚ÐµÑ€ÑŽ
+        loss = self.criterion(logits, labels)
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ’” Loss: {loss.item():.4f}")
+        
+        return {
+            'loss': loss
+        }
+
+    def set_debug_mode(self, debug_forward=False):
+        """
+        Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ forward pass.
+        
+        Args:
+            debug_forward (bool): Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ forward pass
+        """
+        self._debug_forward = debug_forward
+        if debug_forward:
+            print(f"ðŸ› Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ {self.__class__.__name__}")
+            print(f"   ðŸ’” Debug forward: {debug_forward}")
diff --git a/src/metrics/eer.py b/src/metrics/eer.py
index de45458..a9be4a6 100644
--- a/src/metrics/eer.py
+++ b/src/metrics/eer.py
@@ -1,88 +1,138 @@
-import numpy as np
 import torch
-from abc import abstractmethod
-
-class BaseMetric:
-    """
-    Base class for all metrics
-    """
+import numpy as np
+from typing import Dict, Any
 
-    def __init__(self, name=None, *args, **kwargs):
-        self.name = name if name is not None else type(self).__name__
+from src.metrics.base_metric import BaseMetric
 
-    @abstractmethod
-    def __call__(self, **batch):
-        raise NotImplementedError()
 
 class EERMetric(BaseMetric):
     """
-    Equal Error Rate (EER) metric.
-    ÐžÐ¶Ð¸Ð´Ð°ÐµÑ‚ Ð² batch Ð´Ð²Ð° Ð¿Ð¾Ð»Ñ:
-        - 'logits': torch tensor Ñ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð½Ñ‹Ð¼Ð¸ logits
-        - 'labels': torch tensor Ñ Ð¼ÐµÑ‚ÐºÐ°Ð¼Ð¸ (1 â€” bona fide, 0 â€” spoof)
+    Equal Error Rate (EER) metric for audio anti-spoofing.
     """
 
-    def __init__(self, name="eer"):
-        super().__init__(name=name)
-
-    def __call__(self, **batch):
-    
-        logits = batch["logits"]
-        labels = batch["labels"]
-
+    def __init__(self, **kwargs):
+        """
+        Args:
+            **kwargs: additional arguments
+        """
+        super(EERMetric, self).__init__()
+        
+        print("ðŸ“ˆ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ EERMetric...")
         
-        if hasattr(logits, "detach"):
-            logits = logits.detach().cpu().numpy()
-        if hasattr(labels, "detach"):
-            labels = labels.detach().cpu().numpy()
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
+        for key, value in kwargs.items():
+            print(f"   ðŸ“Š {key}: {value}")
+        
+        self.name = "eer"
+        print("âœ… EERMetric Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
 
+    def forward(self, **batch) -> float:
+        """
+        Compute EER metric.
         
-        import torch.nn.functional as F
-        if hasattr(logits, "detach"):
+        Args:
+            **batch: input batch containing scores and labels
             
-            scores = F.softmax(logits, dim=-1)[:, 1]
-        else:
-            
-            logits_tensor = torch.from_numpy(logits)
-            scores_tensor = F.softmax(logits_tensor, dim=-1)
-            scores = scores_tensor[:, 1].numpy()
-
+        Returns:
+            float: EER value
+        """
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸)
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“ˆ EERMetric forward: Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ ÐºÐ»ÑŽÑ‡Ð¸ {list(batch.keys())}")
+            for key, value in batch.items():
+                if isinstance(value, torch.Tensor):
+                    print(f"      {key}: shape={value.shape}, dtype={value.dtype}")
         
-        bona_scores = scores[labels == 1]
-        spoof_scores = scores[labels == 0]
-
-        if len(bona_scores) == 0 or len(spoof_scores) == 0:
+        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ scores Ð¸ labels
+        if 'scores' in batch:
+            scores = batch['scores']
+        elif 'logits' in batch:
+            # Ð•ÑÐ»Ð¸ Ñƒ Ð½Ð°Ñ ÐµÑÑ‚ÑŒ logits, Ð±ÐµÑ€ÐµÐ¼ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÐºÐ»Ð°ÑÑÐ° (spoof)
+            logits = batch['logits']
+            scores = torch.softmax(logits, dim=1)[:, 1]
+        else:
+            print("âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ scores Ð¸Ð»Ð¸ logits Ð² Ð±Ð°Ñ‚Ñ‡Ðµ")
             return 0.0
-
-        eer, _ = self.compute_eer(bona_scores, spoof_scores)
+        
+        labels = batch['labels']
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ€Ð°Ð·Ð¼ÐµÑ€Ñ‹
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“Š Scores: shape={scores.shape}, range=[{scores.min().item():.4f}, {scores.max().item():.4f}]")
+            print(f"   ðŸ“Š Labels: shape={labels.shape}, unique={torch.unique(labels).tolist()}")
+        
+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ EER
+        eer = self._compute_eer(scores, labels)
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“ˆ EER: {eer:.4f}")
+        
         return eer
 
-    @staticmethod
-    def compute_det_curve(target_scores, nontarget_scores):
-        n_scores = target_scores.size + nontarget_scores.size
-        all_scores = np.concatenate((target_scores, nontarget_scores))
-        labels = np.concatenate(
-            (np.ones(target_scores.size), np.zeros(nontarget_scores.size)))
-
-        indices = np.argsort(all_scores, kind='mergesort')
-        labels = labels[indices]
-
-        tar_trial_sums = np.cumsum(labels)
-        nontarget_trial_sums = nontarget_scores.size - \
-            (np.arange(1, n_scores + 1) - tar_trial_sums)
-
-        frr = np.concatenate(
-            (np.atleast_1d(0), tar_trial_sums / target_scores.size))
-        far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums /
-                              nontarget_scores.size))
-        thresholds = np.concatenate(
-            (np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))
-        return frr, far, thresholds
+    def _compute_eer(self, scores: torch.Tensor, labels: torch.Tensor) -> float:
+        """
+        Compute Equal Error Rate.
+        
+        Args:
+            scores (torch.Tensor): prediction scores
+            labels (torch.Tensor): ground truth labels
+            
+        Returns:
+            float: EER value
+        """
+        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² numpy
+        scores_np = scores.detach().cpu().numpy()
+        labels_np = labels.detach().cpu().numpy()
+        
+        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ð¾Ñ€Ð¾Ð³Ð¸
+        thresholds = np.unique(scores_np)
+        
+        # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ FAR Ð¸ FRR Ð´Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð¾Ñ€Ð¾Ð³Ð°
+        far_values = []
+        frr_values = []
+        
+        for threshold in thresholds:
+            # FAR = FP / (FP + TN) = FP / (FP + TN)
+            # FRR = FN / (FN + TP) = FN / (FN + TP)
+            
+            # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ñ: 1 ÐµÑÐ»Ð¸ score >= threshold, Ð¸Ð½Ð°Ñ‡Ðµ 0
+            predictions = (scores_np >= threshold).astype(int)
+            
+            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ confusion matrix
+            tp = np.sum((predictions == 1) & (labels_np == 1))
+            tn = np.sum((predictions == 0) & (labels_np == 0))
+            fp = np.sum((predictions == 1) & (labels_np == 0))
+            fn = np.sum((predictions == 0) & (labels_np == 1))
+            
+            # Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÐ¼ FAR Ð¸ FRR
+            far = fp / (fp + tn) if (fp + tn) > 0 else 0
+            frr = fn / (fn + tp) if (fn + tp) > 0 else 0
+            
+            far_values.append(far)
+            frr_values.append(frr)
+        
+        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ñ‚Ð¾Ñ‡ÐºÑƒ, Ð³Ð´Ðµ FAR â‰ˆ FRR
+        far_values = np.array(far_values)
+        frr_values = np.array(frr_values)
+        
+        # ÐÐ°Ñ…Ð¾Ð´Ð¸Ð¼ Ð¸Ð½Ð´ÐµÐºÑ, Ð³Ð´Ðµ Ñ€Ð°Ð·Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°
+        diff = np.abs(far_values - frr_values)
+        min_idx = np.argmin(diff)
+        
+        # EER - ÑÑ‚Ð¾ ÑÑ€ÐµÐ´Ð½ÐµÐµ FAR Ð¸ FRR Ð² ÑÑ‚Ð¾Ð¹ Ñ‚Ð¾Ñ‡ÐºÐµ
+        eer = (far_values[min_idx] + frr_values[min_idx]) / 2
+        
+        return float(eer)
 
-    @classmethod
-    def compute_eer(cls, bona_scores, spoof_scores):
-        frr, far, thresholds = cls.compute_det_curve(bona_scores, spoof_scores)
-        abs_diffs = np.abs(frr - far)
-        min_index = np.argmin(abs_diffs)
-        eer = np.mean((frr[min_index], far[min_index]))
-        return eer, thresholds[min_index]
\ No newline at end of file
+    def set_debug_mode(self, debug_forward=False):
+        """
+        Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ forward pass.
+        
+        Args:
+            debug_forward (bool): Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ forward pass
+        """
+        self._debug_forward = debug_forward
+        if debug_forward:
+            print(f"ðŸ› Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ {self.__class__.__name__}")
+            print(f"   ðŸ“ˆ Debug forward: {debug_forward}")
\ No newline at end of file
diff --git a/src/model/model.py b/src/model/model.py
index 28a4ce4..6ec39da 100644
--- a/src/model/model.py
+++ b/src/model/model.py
@@ -1,153 +1,113 @@
 import torch
-from torch import nn
+import torch.nn as nn
+from typing import Dict, Any
 
-class mfm_block(nn.Module):
-    def __init__(self, channels):
-        super().__init__()
-        self.channels = channels
-
-    def forward(self, x):
-        partition = self.channels // 2
-        first_batch = x[:, :partition, ...]
-        second_batch = x[:, partition:, ...]
-        output = torch.maximum(first_batch, second_batch)
-        return output
 
 class LCNN(nn.Module):
-    def __init__(self, in_channels=1, num_classes=2, dropout_p=0.3):
-        super().__init__()
-
-        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=5, stride=1, padding=2)
-        self.mfm2 = mfm_block(64)
-        self.dropout2 = nn.Dropout2d(dropout_p)
-        self.MaxPool3 = nn.MaxPool2d(kernel_size=2, stride=2)
-
-        self.conv4 = nn.Conv2d(32, 64, kernel_size=1, stride=1)
-        self.mfm5 = mfm_block(64)
-        self.dropout5 = nn.Dropout2d(dropout_p)
-        self.BatchNorm6 = nn.BatchNorm2d(32)
-
-        self.conv7 = nn.Conv2d(32, 96, kernel_size=3, stride=1, padding=1)
-        self.mfm8 = mfm_block(96)
-        self.dropout8 = nn.Dropout2d(dropout_p)
-
-        self.MaxPool9 = nn.MaxPool2d(kernel_size=2, stride=2)
-        self.BatchNorm10 = nn.BatchNorm2d(48)
-
-        self.conv11 = nn.Conv2d(48, 96, kernel_size=1, stride=1)
-        self.mfm12 = mfm_block(96)
-        self.dropout12 = nn.Dropout2d(dropout_p)
-        self.BatchNorm13 = nn.BatchNorm2d(48)
-
-        self.conv14 = nn.Conv2d(48, 128, kernel_size=3, stride=1, padding=1)
-        self.mfm15 = mfm_block(128)
-        self.dropout15 = nn.Dropout2d(dropout_p)
-
-        self.MaxPool16 = nn.MaxPool2d(kernel_size=2, stride=2)
-
-        self.conv17 = nn.Conv2d(64, 128, kernel_size=1, stride=1)
-        self.mfm18 = mfm_block(128)
-        self.dropout18 = nn.Dropout2d(dropout_p)
-        self.BatchNorm19 = nn.BatchNorm2d(64)
-
-        self.conv20 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)
-        self.mfm21 = mfm_block(64)
-        self.dropout21 = nn.Dropout2d(dropout_p)
-        self.BatchNorm22 = nn.BatchNorm2d(32)
-
-        self.conv23 = nn.Conv2d(32, 64, kernel_size=1, stride=1)
-        self.mfm24 = mfm_block(64)
-        self.dropout24 = nn.Dropout2d(dropout_p)
-        self.BatchNorm25 = nn.BatchNorm2d(32)
-
-        self.conv26 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
-        self.mfm27 = mfm_block(64)
-        self.dropout27 = nn.Dropout2d(dropout_p)
-
-        self.MaxPool28 = nn.MaxPool2d(kernel_size=2, stride=2)
-
-        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
-        self.fc29 = nn.Linear(32, 160)
-        self.dropout29 = nn.Dropout(dropout_p)
-        self.mfm30 = mfm_block(160)
-        self.BatchNorm31 = nn.BatchNorm1d(80)
-        self.fc32 = nn.Linear(80, num_classes)
+    """
+    Light CNN model for audio anti-spoofing.
+    """
+
+    def __init__(self, num_classes=2, **kwargs):
+        """
+        Args:
+            num_classes (int): number of output classes
+            **kwargs: additional arguments
+        """
+        super(LCNN, self).__init__()
         
-        # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð²
-        self._initialize_weights()
-
-    def _initialize_weights(self):
-        """Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÑÐ¾Ð² Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ"""
-        for m in self.modules():
-            if isinstance(m, nn.Conv2d):
-                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
-                if m.bias is not None:
-                    nn.init.constant_(m.bias, 0)
-            elif isinstance(m, nn.BatchNorm2d):
-                nn.init.constant_(m.weight, 1)
-                nn.init.constant_(m.bias, 0)
-            elif isinstance(m, nn.Linear):
-                nn.init.normal_(m.weight, 0, 0.01)
-                nn.init.constant_(m.bias, 0)
-
-    def forward(self, data_object, **kwargs):
-        x = data_object
-        x = self.conv1(x)
-        x = self.mfm2(x)
-        x = self.dropout2(x)
-        x = self.MaxPool3(x)
-
-        x = self.conv4(x)
-        x = self.mfm5(x)
-        x = self.dropout5(x)
-        x = self.BatchNorm6(x)
-
-        x = self.conv7(x)
-        x = self.mfm8(x)
-        x = self.dropout8(x)
-
-        x = self.MaxPool9(x)
-        x = self.BatchNorm10(x)
-
-        x = self.conv11(x)
-        x = self.mfm12(x)
-        x = self.dropout12(x)
-        x = self.BatchNorm13(x)
-
-        x = self.conv14(x)
-        x = self.mfm15(x)
-        x = self.dropout15(x)
-
-        x = self.MaxPool16(x)
-
-        x = self.conv17(x)
-        x = self.mfm18(x)
-        x = self.dropout18(x)
-        x = self.BatchNorm19(x)
-
-        x = self.conv20(x)
-        x = self.mfm21(x)
-        x = self.dropout21(x)
-        x = self.BatchNorm22(x)
-
-        x = self.conv23(x)
-        x = self.mfm24(x)
-        x = self.dropout24(x)
-        x = self.BatchNorm25(x)
-
-        x = self.conv26(x)
-        x = self.mfm27(x)
-        x = self.dropout27(x)
+        self.num_classes = num_classes
+        
+        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ñƒ
+        self.features = nn.Sequential(
+            # ÐŸÐµÑ€Ð²Ñ‹Ð¹ Ð±Ð»Ð¾Ðº
+            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),
+            nn.BatchNorm2d(64),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+            
+            # Ð’Ñ‚Ð¾Ñ€Ð¾Ð¹ Ð±Ð»Ð¾Ðº
+            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(128),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+            
+            # Ð¢Ñ€ÐµÑ‚Ð¸Ð¹ Ð±Ð»Ð¾Ðº
+            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(256),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+            
+            # Ð§ÐµÑ‚Ð²ÐµÑ€Ñ‚Ñ‹Ð¹ Ð±Ð»Ð¾Ðº
+            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
+            nn.BatchNorm2d(512),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(kernel_size=2, stride=2),
+        )
+        
+        # Global Average Pooling
+        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
+        
+        # ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ‚Ð¾Ñ€
+        self.classifier = nn.Sequential(
+            nn.Dropout(0.5),
+            nn.Linear(512, 256),
+            nn.ReLU(inplace=True),
+            nn.Dropout(0.5),
+            nn.Linear(256, num_classes)
+        )
+
+    def forward(self, **batch) -> Dict[str, torch.Tensor]:
+        """
+        Forward pass of the model.
+        
+        Args:
+            **batch: input batch containing tensors
+            
+        Returns:
+            Dict[str, torch.Tensor]: model outputs
+        """
+        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
+        if 'spectrogram' in batch:
+            x = batch['spectrogram']
+        elif 'data_object' in batch:
+            x = batch['data_object']
+        else:
+            # Ð‘ÐµÑ€ÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ‚ÐµÐ½Ð·Ð¾Ñ€ Ð¸Ð· Ð±Ð°Ñ‚Ñ‡Ð°
+            x = next(iter(batch.values()))
+        
+        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð¼ÐµÑŽÑ‚ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½ÑƒÑŽ Ñ„Ð¾Ñ€Ð¼Ñƒ
+        if x.dim() == 3:
+            x = x.unsqueeze(1)  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ ÐºÐ°Ð½Ð°Ð»
+        elif x.dim() == 2:
+            x = x.unsqueeze(0).unsqueeze(0)  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ batch Ð¸ ÐºÐ°Ð½Ð°Ð»
+        
+        # ÐŸÑ€Ð¾Ñ…Ð¾Ð´Ð¸Ð¼ Ñ‡ÐµÑ€ÐµÐ· ÑÐ»Ð¾Ð¸
+        x = self.features(x)
+        
+        x = self.global_avg_pool(x)
+        x = x.view(x.size(0), -1)
+        
+        x = self.classifier(x)
+        
+        # Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ñ‹
+        outputs = {
+            'logits': x
+        }
+        
+        return outputs
 
-        x = self.MaxPool28(x)
 
-        x = self.adaptive_pool(x)
-        x = x.view(x.size(0), -1)
-        x = self.fc29(x)
-        x = self.dropout29(x)
-        x = x.unsqueeze(-1).unsqueeze(-1)
-        x = self.mfm30(x)
-        x = x.squeeze(-1).squeeze(-1)
-        x = self.BatchNorm31(x)
-        logits = self.fc32(x)
-        return {"logits": logits}
\ No newline at end of file
+# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸ Ñ hydra
+def create_model(**kwargs) -> LCNN:
+    """
+    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸ LCNN.
+    
+    Args:
+        **kwargs: Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
+        
+    Returns:
+        LCNN: ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼Ð¾Ð´ÐµÐ»Ð¸
+    """
+    model = LCNN(**kwargs)
+    return model
\ No newline at end of file
diff --git a/src/trainer/base_trainer.py b/src/trainer/base_trainer.py
index e35ffba..0d4e7a1 100644
--- a/src/trainer/base_trainer.py
+++ b/src/trainer/base_trainer.py
@@ -148,250 +148,224 @@ class BaseTrainer:
         """
         try:
             self._train_process()
-        except KeyboardInterrupt as e:
-            self.logger.info("Saving model on keyboard interrupt")
-            self._save_checkpoint(self._last_epoch, save_best=False)
-            raise e
+        except KeyboardInterrupt:
+            self._save_checkpoint(self._last_epoch, save_best=False, only_best=True)
+            raise
+        except Exception as e:
+            raise
 
     def _train_process(self):
         """
-        Full training logic:
-
-        Training model for an epoch, evaluating it on non-train partitions,
-        and monitoring the performance improvement (for early stopping
-        and saving the best checkpoint).
+        Full training logic.
         """
         not_improved_count = 0
         for epoch in range(self.start_epoch, self.epochs + 1):
             self._last_epoch = epoch
-            result = self._train_epoch(epoch)
-
-            # save logged information into logs dict
-            logs = {"epoch": epoch}
-            logs.update(result)
+            self._train_epoch(epoch)
 
-            # print logged information to the screen
-            for key, value in logs.items():
-                self.logger.info(f"    {key:15s}: {value}")
+            # Ð’Ð°Ð»Ð¸Ð´Ð¸Ñ€ÑƒÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° val, test Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð´Ð»Ñ Ð¸Ð½Ñ„ÐµÑ€ÐµÐ½ÑÐ°
+            if "val" in self.evaluation_dataloaders:
+                val_logs = {}
+                dataloader = self.evaluation_dataloaders["val"]
+                val_part_logs = self._evaluation_epoch(epoch, "val", dataloader)
+                val_logs.update(val_part_logs)
 
-            # evaluate model performance according to configured metric,
-            # save best checkpoint as model_best
-            best, stop_process, not_improved_count = self._monitor_performance(
-                logs, not_improved_count
-            )
+                # log best so far
+                if self.mnt_mode != "off":
+                    improved = self._monitor_performance(val_logs, not_improved_count)
+                    if improved:
+                        not_improved_count = 0
+                    else:
+                        not_improved_count += 1
 
-            if epoch % self.save_period == 0 or best:
-                self._save_checkpoint(epoch, save_best=best, only_best=True)
+                if self.mnt_mode != "off" and not_improved_count > self.early_stop:
+                    break
 
-            if stop_process:  # early_stop
-                break
+            if epoch % self.save_period == 0:
+                self._save_checkpoint(epoch, save_best=False)
 
     def _train_epoch(self, epoch):
         """
-        Training logic for an epoch, including logging and evaluation on
-        non-train partitions.
+        Training logic for an epoch.
 
         Args:
-            epoch (int): current training epoch.
-        Returns:
-            logs (dict): logs that contain the average loss and metric in
-                this epoch.
+            epoch (int): Current epoch number.
         """
-        self.is_train = True
         self.model.train()
         self.train_metrics.reset()
-        self.writer.set_step((epoch - 1) * self.epoch_len)
-        self.writer.add_scalar("epoch", epoch)
-        for batch_idx, batch in enumerate(
-            tqdm(self.train_dataloader, desc="train", total=self.epoch_len)
-        ):
+        
+        pbar = tqdm(self.train_dataloader, desc=f"Train Epoch {epoch}")
+        for batch_idx, batch in enumerate(pbar):
             try:
-                batch = self.process_batch(
-                    batch,
-                    metrics=self.train_metrics,
-                )
-            except torch.cuda.OutOfMemoryError as e:
-                if self.skip_oom:
-                    self.logger.warning("OOM on batch. Skipping batch.")
-                    torch.cuda.empty_cache()  # free some memory
+                batch = self.process_batch(batch, self.train_metrics)
+                self._log_batch(batch_idx, batch, "train")
+                
+                # Ð’Ñ‹Ð²Ð¾Ð´Ð¸Ð¼ Ð»Ð¾ÑÑ Ð² ÐºÐ¾Ð½ÑÐ¾Ð»ÑŒ ÐºÐ°Ð¶Ð´Ñ‹Ðµ 50 Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
+                if batch_idx % 50 == 0:
+                    loss_key = self.config.writer.loss_names[0]
+                    current_loss = self.train_metrics.avg(loss_key)
+                    print(f"[Batch {batch_idx}] Loss: {current_loss:.6f}")
+                
+                # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð² ÑÐµÑ€ÐµÐ´Ð¸Ð½Ðµ ÑÐ¿Ð¾Ñ…Ð¸
+                if batch_idx == len(self.train_dataloader) // 2 and "val" in self.evaluation_dataloaders:
+                    self._quick_validation(epoch, "val", self.evaluation_dataloaders["val"])
+                    
+            except RuntimeError as e:
+                if "out of memory" in str(e) and self.skip_oom:
+                    if hasattr(torch.cuda, 'empty_cache'):
+                        torch.cuda.empty_cache()
                     continue
                 else:
                     raise e
+                    
+        self._log_scalars(self.train_metrics)
+
+    def _quick_validation(self, epoch, part, dataloader):
+        """
+        Ð‘Ñ‹ÑÑ‚Ñ€Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ‚Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ EER Ð² ÑÐµÑ€ÐµÐ´Ð¸Ð½Ðµ ÑÐ¿Ð¾Ñ…Ð¸.
 
-            self.train_metrics.update("grad_norm", self._get_grad_norm())
-
-            # log current results
-            if batch_idx % self.log_step == 0:
-                self.writer.set_step((epoch - 1) * self.epoch_len + batch_idx)
-                self.logger.debug(
-                    "Train Epoch: {} {} Loss: {:.6f}".format(
-                        epoch, self._progress(batch_idx), batch["loss"].item()
-                    )
-                )
-                self.writer.add_scalar(
-                    "learning rate", self.lr_scheduler.get_last_lr()[0]
-                )
-                self._log_scalars(self.train_metrics)
-                self._log_batch(batch_idx, batch)
-                # we don't want to reset train metrics at the start of every epoch
-                # because we are interested in recent train metrics
-                last_train_metrics = self.train_metrics.result()
-                self.train_metrics.reset()
-            if batch_idx + 1 >= self.epoch_len:
-                break
-
-        logs = last_train_metrics
-
-        # Run val/test
-        for part, dataloader in self.evaluation_dataloaders.items():
-            val_logs = self._evaluation_epoch(epoch, part, dataloader)
-            logs.update(**{f"{part}_{name}": value for name, value in val_logs.items()})
-
-        return logs
+        Args:
+            epoch (int): Current epoch number.
+            part (str): Name of the data part.
+            dataloader (DataLoader): Dataloader for validation.
+        """
+        self.model.eval()
+        temp_metrics = MetricTracker(
+            *self.config.writer.loss_names,
+            *[m.name for m in self.metrics["inference"]],
+            writer=None,  # ÐÐµ Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð² writer Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸
+        )
+        
+        with torch.no_grad():
+            # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð¿ÐµÑ€Ð²Ñ‹Ðµ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð±Ð°Ñ‚Ñ‡ÐµÐ¹ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸
+            num_batches = min(10, len(dataloader))  # ÐœÐ°ÐºÑÐ¸Ð¼ÑƒÐ¼ 10 Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
+            for batch_idx, batch in enumerate(dataloader):
+                if batch_idx >= num_batches:
+                    break
+                    
+                try:
+                    batch = self.process_batch(batch, temp_metrics)
+                except RuntimeError as e:
+                    if "out of memory" in str(e) and self.skip_oom:
+                        if hasattr(torch.cuda, 'empty_cache'):
+                            torch.cuda.empty_cache()
+                        continue
+                    else:
+                        raise e
 
     def _evaluation_epoch(self, epoch, part, dataloader):
         """
-        Evaluate model on the partition after training for an epoch.
+        Validate after training an epoch.
 
         Args:
-            epoch (int): current training epoch.
-            part (str): partition to evaluate on
-            dataloader (DataLoader): dataloader for the partition.
+            epoch (int): Current epoch number.
+            part (str): Name of the data part.
+            dataloader (DataLoader): Dataloader for validation.
+
         Returns:
-            logs (dict): logs that contain the information about evaluation.
+            dict: Dictionary with validation logs.
         """
-        self.is_train = False
         self.model.eval()
         self.evaluation_metrics.reset()
+        
         with torch.no_grad():
-            for batch_idx, batch in tqdm(
-                enumerate(dataloader),
-                desc=part,
-                total=len(dataloader),
-            ):
-                batch = self.process_batch(
-                    batch,
-                    metrics=self.evaluation_metrics,
-                )
-            self.writer.set_step(epoch * self.epoch_len, part)
-            self._log_scalars(self.evaluation_metrics)
-            self._log_batch(
-                batch_idx, batch, part
-            )  # log only the last batch during inference
-
+            pbar = tqdm(dataloader, desc=f"Validation {part} Epoch {epoch}")
+            for batch_idx, batch in enumerate(pbar):
+                try:
+                    batch = self.process_batch(batch, self.evaluation_metrics)
+                    self._log_batch(batch_idx, batch, part)
+                        
+                except RuntimeError as e:
+                    if "out of memory" in str(e) and self.skip_oom:
+                        if hasattr(torch.cuda, 'empty_cache'):
+                            torch.cuda.empty_cache()
+                        continue
+                    else:
+                        raise e
+
+        self._log_scalars(self.evaluation_metrics)
         return self.evaluation_metrics.result()
 
     def _monitor_performance(self, logs, not_improved_count):
         """
-        Check if there is an improvement in the metrics. Used for early
-        stopping and saving the best checkpoint.
+        Monitor the performance and save the best model.
 
         Args:
-            logs (dict): logs after training and evaluating the model for
-                an epoch.
-            not_improved_count (int): the current number of epochs without
-                improvement.
+            logs (dict): Dictionary with validation logs.
+            not_improved_count (int): Number of epochs without improvement.
+
         Returns:
-            best (bool): if True, the monitored metric has improved.
-            stop_process (bool): if True, stop the process (early stopping).
-                The metric did not improve for too much epochs.
-            not_improved_count (int): updated number of epochs without
-                improvement.
-        """
-        best = False
-        stop_process = False
-        if self.mnt_mode != "off":
-            try:
-                # check whether model performance improved or not,
-                # according to specified metric(mnt_metric)
-                if self.mnt_mode == "min":
-                    improved = logs[self.mnt_metric] <= self.mnt_best
-                elif self.mnt_mode == "max":
-                    improved = logs[self.mnt_metric] >= self.mnt_best
-                else:
-                    improved = False
-            except KeyError:
-                self.logger.warning(
-                    f"Warning: Metric '{self.mnt_metric}' is not found. "
-                    "Model performance monitoring is disabled."
-                )
-                self.mnt_mode = "off"
-                improved = False
-
-            if improved:
-                self.mnt_best = logs[self.mnt_metric]
-                not_improved_count = 0
-                best = True
-            else:
-                not_improved_count += 1
-
-            if not_improved_count >= self.early_stop:
-                self.logger.info(
-                    "Validation performance didn't improve for {} epochs. "
-                    "Training stops.".format(self.early_stop)
-                )
-                stop_process = True
-        return best, stop_process, not_improved_count
+            bool: True if the model improved.
+        """
+        if self.mnt_mode == "off":
+            return False
+
+        try:
+            current = logs[self.mnt_metric]
+        except KeyError:
+            return False
+
+        if self.mnt_mode == "min":
+            improved = current < self.mnt_best
+        else:
+            improved = current > self.mnt_best
+
+        if improved:
+            self.mnt_best = current
+            self._save_checkpoint(self._last_epoch, save_best=True)
+
+        return improved
 
     def move_batch_to_device(self, batch):
         """
-        Move all necessary tensors to the device.
+        Move batch to device.
 
         Args:
-            batch (dict): dict-based batch containing the data from
-                the dataloader.
+            batch (dict): Batch to move to device.
+
         Returns:
-            batch (dict): dict-based batch containing the data from
-                the dataloader with some of the tensors on the device.
+            dict: Batch on device.
         """
-        for tensor_for_device in self.cfg_trainer.device_tensors:
-            batch[tensor_for_device] = batch[tensor_for_device].to(self.device)
+        for k, v in batch.items():
+            if isinstance(v, torch.Tensor):
+                batch[k] = v.to(self.device)
         return batch
 
     def transform_batch(self, batch):
         """
-        Transforms elements in batch. Like instance transform inside the
-        BaseDataset class, but for the whole batch. Improves pipeline speed,
-        especially if used with a GPU.
-
-        Each tensor in a batch undergoes its own transform defined by the key.
+        Transform batch using batch transforms.
 
         Args:
-            batch (dict): dict-based batch containing the data from
-                the dataloader.
+            batch (dict): Batch to transform.
+
         Returns:
-            batch (dict): dict-based batch containing the data from
-                the dataloader (possibly transformed via batch transform).
-        """
-        # do batch transforms on device
-        transform_type = "train" if self.is_train else "inference"
-        transforms = self.batch_transforms.get(transform_type)
-        if transforms is not None:
-            for transform_name in transforms.keys():
-                batch[transform_name] = transforms[transform_name](
-                    batch[transform_name]
-                )
+            dict: Transformed batch.
+        """
+        if self.batch_transforms is not None:
+            for transform_name, transform in self.batch_transforms.items():
+                if transform_name in batch:
+                    batch[transform_name] = transform(batch[transform_name])
         return batch
 
     def _clip_grad_norm(self):
         """
-        Clips the gradient norm by the value defined in
-        config.trainer.max_grad_norm
+        Clip gradient norm.
         """
-        if self.config["trainer"].get("max_grad_norm", None) is not None:
+        if self.cfg_trainer.get("grad_clip_norm") is not None:
             clip_grad_norm_(
-                self.model.parameters(), self.config["trainer"]["max_grad_norm"]
+                self.model.parameters(), self.cfg_trainer.grad_clip_norm
             )
 
     @torch.no_grad()
     def _get_grad_norm(self, norm_type=2):
         """
-        Calculates the gradient norm for logging.
+        Get gradient norm.
 
         Args:
-            norm_type (float | str | None): the order of the norm.
+            norm_type (int): Type of norm.
+
         Returns:
-            total_norm (float): the calculated norm.
+            float: Gradient norm.
         """
         parameters = self.model.parameters()
         if isinstance(parameters, torch.Tensor):
@@ -401,17 +375,14 @@ class BaseTrainer:
             torch.stack([torch.norm(p.grad.detach(), norm_type) for p in parameters]),
             norm_type,
         )
-        return total_norm.item()
+        return total_norm
 
     def _progress(self, batch_idx):
         """
-        Calculates the percentage of processed batch within the epoch.
+        Print progress.
 
         Args:
-            batch_idx (int): the current batch index.
-        Returns:
-            progress (str): contains current step and percentage
-                within the epoch.
+            batch_idx (int): Current batch index.
         """
         base = "[{}/{} ({:.0f}%)]"
         if hasattr(self.train_dataloader, "n_samples"):
@@ -425,129 +396,106 @@ class BaseTrainer:
     @abstractmethod
     def _log_batch(self, batch_idx, batch, mode="train"):
         """
-        Abstract method. Should be defined in the nested Trainer Class.
-
         Log data from batch. Calls self.writer.add_* to log data
         to the experiment tracker.
 
         Args:
-            batch_idx (int): index of the current batch.
-            batch (dict): dict-based batch after going through
-                the 'process_batch' function.
-            mode (str): train or inference. Defines which logging
-                rules to apply.
+            batch_idx (int): Current batch index.
+            batch (dict): Batch data.
+            mode (str): Mode (train or validation).
         """
-        return NotImplementedError()
+        pass
 
     def _log_scalars(self, metric_tracker: MetricTracker):
         """
-        Wrapper around the writer 'add_scalar' to log all metrics.
+        Log scalars to the experiment tracker.
 
         Args:
-            metric_tracker (MetricTracker): calculated metrics.
+            metric_tracker (MetricTracker): Metric tracker.
         """
-        if self.writer is None:
-            return
-        for metric_name in metric_tracker.keys():
-            self.writer.add_scalar(f"{metric_name}", metric_tracker.avg(metric_name))
+        for metric_name, metric_value in metric_tracker.result().items():
+            self.writer.add_scalar(metric_name, metric_value)
 
     def _save_checkpoint(self, epoch, save_best=False, only_best=False):
         """
-        Save the checkpoints.
+        Save checkpoint.
 
         Args:
-            epoch (int): current epoch number.
-            save_best (bool): if True, rename the saved checkpoint to 'model_best.pth'.
-            only_best (bool): if True and the checkpoint is the best, save it only as
-                'model_best.pth'(do not duplicate the checkpoint as
-                checkpoint-epochEpochNumber.pth)
+            epoch (int): Current epoch number.
+            save_best (bool): Whether to save the best model.
+            only_best (bool): Whether to save only the best model.
         """
         arch = type(self.model).__name__
+
         state = {
             "arch": arch,
             "epoch": epoch,
             "state_dict": self.model.state_dict(),
             "optimizer": self.optimizer.state_dict(),
-            "lr_scheduler": self.lr_scheduler.state_dict(),
             "monitor_best": self.mnt_best,
             "config": self.config,
         }
-        filename = str(self.checkpoint_dir / f"checkpoint-epoch{epoch}.pth")
-        if not (only_best and save_best):
-            torch.save(state, filename)
-            if self.config.writer.log_checkpoints:
-                self.writer.add_checkpoint(filename, str(self.checkpoint_dir.parent))
-            self.logger.info(f"Saving checkpoint: {filename} ...")
+
+        if self.lr_scheduler is not None:
+            state["lr_scheduler"] = self.lr_scheduler.state_dict()
+
+        filename = str(self.checkpoint_dir / "checkpoint-epoch{}.pth".format(epoch))
+        if not (self.checkpoint_dir).exists():
+            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
+
         if save_best:
             best_path = str(self.checkpoint_dir / "model_best.pth")
             torch.save(state, best_path)
-            if self.config.writer.log_checkpoints:
-                self.writer.add_checkpoint(best_path, str(self.checkpoint_dir.parent))
-            self.logger.info("Saving current best: model_best.pth ...")
+            del state["optimizer"], state["lr_scheduler"], state["config"]
+            torch.save(state, best_path + ".tmp")
+            import os
+            os.replace(best_path + ".tmp", best_path)
+        elif not only_best:
+            torch.save(state, filename)
+            del state["optimizer"], state["lr_scheduler"], state["config"]
+            torch.save(state, filename + ".tmp")
+            import os
+            os.replace(filename + ".tmp", filename)
 
     def _resume_checkpoint(self, resume_path):
         """
-        Resume from a saved checkpoint (in case of server crash, etc.).
-        The function loads state dicts for everything, including model,
-        optimizers, etc.
-
-        Notice that the checkpoint should be located in the current experiment
-        saved directory (where all checkpoints are saved in '_save_checkpoint').
+        Resume from saved checkpoint.
 
         Args:
-            resume_path (str): Path to the checkpoint to be resumed.
+            resume_path (str): Path to checkpoint.
         """
         resume_path = str(resume_path)
-        self.logger.info(f"Loading checkpoint: {resume_path} ...")
-        checkpoint = torch.load(resume_path, self.device)
+        checkpoint = torch.load(resume_path, map_location=self.device)
         self.start_epoch = checkpoint["epoch"] + 1
         self.mnt_best = checkpoint["monitor_best"]
 
         # load architecture params from checkpoint.
         if checkpoint["config"]["model"] != self.config["model"]:
             self.logger.warning(
-                "Warning: Architecture configuration given in the config file is different from that "
-                "of the checkpoint. This may yield an exception when state_dict is loaded."
+                "Warning: Architecture configuration given in config file is different from that of checkpoint. "
+                "This may create an exception while state_dict is being loaded."
             )
         self.model.load_state_dict(checkpoint["state_dict"])
 
         # load optimizer state from checkpoint only when optimizer type is not changed.
-        if (
-            checkpoint["config"]["optimizer"] != self.config["optimizer"]
-            or checkpoint["config"]["lr_scheduler"] != self.config["lr_scheduler"]
-        ):
+        if checkpoint["config"]["optimizer"] != self.config["optimizer"]:
             self.logger.warning(
-                "Warning: Optimizer or lr_scheduler given in the config file is different "
-                "from that of the checkpoint. Optimizer and scheduler parameters "
-                "are not resumed."
+                "Warning: Optimizer or lr_scheduler given in config file is different "
+                "from that of checkpoint. Optimizer parameters not being resumed."
             )
         else:
             self.optimizer.load_state_dict(checkpoint["optimizer"])
-            self.lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
 
-        self.logger.info(
-            f"Checkpoint loaded. Resume training from epoch {self.start_epoch}"
-        )
+        if self.lr_scheduler is not None and "lr_scheduler" in checkpoint:
+            self.lr_scheduler.load_state_dict(checkpoint["lr_scheduler"])
 
     def _from_pretrained(self, pretrained_path):
         """
-        Init model with weights from pretrained pth file.
-
-        Notice that 'pretrained_path' can be any path on the disk. It is not
-        necessary to locate it in the experiment saved dir. The function
-        initializes only the model.
+        Load pretrained model.
 
         Args:
-            pretrained_path (str): path to the model state dict.
+            pretrained_path (str): Path to pretrained model.
         """
         pretrained_path = str(pretrained_path)
-        if hasattr(self, "logger"):  # to support both trainer and inferencer
-            self.logger.info(f"Loading model weights from: {pretrained_path} ...")
-        else:
-            print(f"Loading model weights from: {pretrained_path} ...")
-        checkpoint = torch.load(pretrained_path, self.device)
-
-        if checkpoint.get("state_dict") is not None:
-            self.model.load_state_dict(checkpoint["state_dict"])
-        else:
-            self.model.load_state_dict(checkpoint)
\ No newline at end of file
+        checkpoint = torch.load(pretrained_path, map_location=self.device)
+        self.model.load_state_dict(checkpoint["state_dict"])
\ No newline at end of file
diff --git a/src/trainer/trainer.py b/src/trainer/trainer.py
index 3313fea..f2b2faa 100644
--- a/src/trainer/trainer.py
+++ b/src/trainer/trainer.py
@@ -61,9 +61,9 @@ class Trainer(BaseTrainer):
         for met in metric_funcs:
             if met.name != "eer":
                 try:
-                    metrics.update(met.name, met(**batch))
+                    metric_value = met(**batch)
+                    metrics.update(met.name, metric_value)
                 except Exception as e:
-                    print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð¼ÐµÑ‚Ñ€Ð¸ÐºÐµ {met.name}: {e}")
                     continue
         return batch
 
@@ -72,4 +72,13 @@ class Trainer(BaseTrainer):
         Log data from batch. Calls self.writer.add_* to log data
         to the experiment tracker.
         """
-        pass
\ No newline at end of file
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ Ð±Ð°Ñ‚Ñ‡Ðµ Ð´Ð»Ñ writer
+        if self.writer is not None:
+            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ learning rate
+            if mode == "train" and self.lr_scheduler is not None:
+                self.writer.add_scalar("learning_rate", self.lr_scheduler.get_last_lr()[0])
+            
+            # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð³Ñ€Ð°Ð´Ð¸ÐµÐ½Ñ‚Ð½ÑƒÑŽ Ð½Ð¾Ñ€Ð¼Ñƒ
+            if mode == "train":
+                grad_norm = self._get_grad_norm()
+                self.writer.add_scalar("grad_norm", grad_norm)
\ No newline at end of file
diff --git a/src/transforms/__init__.py b/src/transforms/__init__.py
index bea0123..1c032b1 100644
--- a/src/transforms/__init__.py
+++ b/src/transforms/__init__.py
@@ -1,3 +1,3 @@
 from src.transforms.normalize import Normalize
 from src.transforms.scale import RandomScale1D
-from src.transforms.stft import AudioFrontend
\ No newline at end of file
+from src.transforms.stft import STFTTransform, MelSpectrogramTransform
\ No newline at end of file
diff --git a/src/transforms/stft.py b/src/transforms/stft.py
index 954ff81..f7e362f 100644
--- a/src/transforms/stft.py
+++ b/src/transforms/stft.py
@@ -1,37 +1,177 @@
 import torch
 import torch.nn as nn
+import torchaudio
+from typing import Dict, Any
 
 
-def audio_frontend(waveform):
+class STFTTransform(nn.Module):
+    """
+    Short-Time Fourier Transform (STFT) for audio processing.
+    """
 
-    n_fft = 1024
-    hop_length = 256
-    win_length = 1024
+    def __init__(self, n_fft=1024, hop_length=512, win_length=1024, **kwargs):
+        """
+        Args:
+            n_fft (int): FFT window size
+            hop_length (int): Number of samples between successive frames
+            win_length (int): Window size
+            **kwargs: additional arguments
+        """
+        super(STFTTransform, self).__init__()
+        
+        print("ðŸŽµ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ STFTTransform...")
+        print(f"   ðŸ“Š n_fft: {n_fft}")
+        print(f"   ðŸ“Š hop_length: {hop_length}")
+        print(f"   ðŸ“Š win_length: {win_length}")
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
+        for key, value in kwargs.items():
+            print(f"   ðŸ“Š {key}: {value}")
+        
+        self.n_fft = n_fft
+        self.hop_length = hop_length
+        self.win_length = win_length
+        
+        print("âœ… STFTTransform Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
 
-    waveform = waveform.squeeze()
+    def forward(self, audio: torch.Tensor) -> torch.Tensor:
+        """
+        Apply STFT to audio signal.
+        
+        Args:
+            audio (torch.Tensor): input audio tensor
+            
+        Returns:
+            torch.Tensor: STFT spectrogram
+        """
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸)
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸŽµ STFTTransform forward: audio shape={audio.shape}")
+            print(f"      Audio range: [{audio.min().item():.4f}, {audio.max().item():.4f}]")
+            print(f"      Audio dtype: {audio.dtype}")
+        
+        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð°ÑƒÐ´Ð¸Ð¾ Ð¸Ð¼ÐµÐµÑ‚ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½ÑƒÑŽ Ñ„Ð¾Ñ€Ð¼Ñƒ
+        if audio.dim() == 1:
+            audio = audio.unsqueeze(0)  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ batch dimension
+        elif audio.dim() == 3:
+            audio = audio.squeeze(1)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð»Ð¸ÑˆÐ½Ð¸Ð¹ ÐºÐ°Ð½Ð°Ð»
+        
+        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ STFT
+        stft_output = torch.stft(
+            audio,
+            n_fft=self.n_fft,
+            hop_length=self.hop_length,
+            win_length=self.win_length,
+            return_complex=True,
+            window=torch.hann_window(self.win_length).to(audio.device)
+        )
+        
+        # ÐšÐ¾Ð½Ð²ÐµÑ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð² ÑÐ¿ÐµÐºÑ‚Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ
+        spectrogram = torch.abs(stft_output)
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“Š STFT output shape: {stft_output.shape}")
+            print(f"   ðŸ“Š Spectrogram shape: {spectrogram.shape}")
+            print(f"   ðŸ“Š Spectrogram range: [{spectrogram.min().item():.4f}, {spectrogram.max().item():.4f}]")
+        
+        return spectrogram
 
-    stft = torch.stft(
-        waveform,
-        n_fft=n_fft,
-        hop_length=hop_length,
-        win_length=win_length,
-        window=torch.hann_window(win_length, device=waveform.device),
-        return_complex=True
-    )
+    def set_debug_mode(self, debug_forward=False):
+        """
+        Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ forward pass.
+        
+        Args:
+            debug_forward (bool): Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ forward pass
+        """
+        self._debug_forward = debug_forward
+        if debug_forward:
+            print(f"ðŸ› Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ {self.__class__.__name__}")
+            print(f"   ðŸŽµ Debug forward: {debug_forward}")
 
-    magnitude = torch.abs(stft)
-    log_magnitude = torch.log(magnitude + 1e-8)
-    log_magnitude = log_magnitude.unsqueeze(0)
-    return log_magnitude
 
+class MelSpectrogramTransform(nn.Module):
+    """
+    Mel Spectrogram transform for audio processing.
+    """
 
-class AudioFrontend(nn.Module):
-    
-    def __init__(self):
-        super().__init__()
-    
-    def forward(self, waveform):
-      
+    def __init__(self, sample_rate=16000, n_fft=1024, hop_length=512, n_mels=80, **kwargs):
+        """
+        Args:
+            sample_rate (int): Audio sample rate
+            n_fft (int): FFT window size
+            hop_length (int): Number of samples between successive frames
+            n_mels (int): Number of mel filter banks
+            **kwargs: additional arguments
+        """
+        super(MelSpectrogramTransform, self).__init__()
+        
+        print("ðŸŽµ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ MelSpectrogramTransform...")
+        print(f"   ðŸ“Š sample_rate: {sample_rate}")
+        print(f"   ðŸ“Š n_fft: {n_fft}")
+        print(f"   ðŸ“Š hop_length: {hop_length}")
+        print(f"   ðŸ“Š n_mels: {n_mels}")
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
+        for key, value in kwargs.items():
+            print(f"   ðŸ“Š {key}: {value}")
+        
+        self.sample_rate = sample_rate
+        self.n_fft = n_fft
+        self.hop_length = hop_length
+        self.n_mels = n_mels
+        
+        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ mel spectrogram transform
+        self.mel_transform = torchaudio.transforms.MelSpectrogram(
+            sample_rate=sample_rate,
+            n_fft=n_fft,
+            hop_length=hop_length,
+            n_mels=n_mels,
+            window_fn=torch.hann_window
+        )
+        
+        print("âœ… MelSpectrogramTransform Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½")
 
+    def forward(self, audio: torch.Tensor) -> torch.Tensor:
+        """
+        Apply Mel Spectrogram transform to audio signal.
+        
+        Args:
+            audio (torch.Tensor): input audio tensor
+            
+        Returns:
+            torch.Tensor: Mel spectrogram
+        """
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸)
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸŽµ MelSpectrogramTransform forward: audio shape={audio.shape}")
+            print(f"      Audio range: [{audio.min().item():.4f}, {audio.max().item():.4f}]")
+            print(f"      Audio dtype: {audio.dtype}")
+        
+        # Ð£Ð±ÐµÐ¶Ð´Ð°ÐµÐ¼ÑÑ, Ñ‡Ñ‚Ð¾ Ð°ÑƒÐ´Ð¸Ð¾ Ð¸Ð¼ÐµÐµÑ‚ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½ÑƒÑŽ Ñ„Ð¾Ñ€Ð¼Ñƒ
+        if audio.dim() == 1:
+            audio = audio.unsqueeze(0)  # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ batch dimension
+        elif audio.dim() == 3:
+            audio = audio.squeeze(1)  # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð»Ð¸ÑˆÐ½Ð¸Ð¹ ÐºÐ°Ð½Ð°Ð»
+        
+        # ÐŸÑ€Ð¸Ð¼ÐµÐ½ÑÐµÐ¼ mel spectrogram transform
+        mel_spectrogram = self.mel_transform(audio)
+        
+        # Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÐ¼ Ð²Ñ‹Ñ…Ð¾Ð´Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ
+        if hasattr(self, '_debug_forward') and self._debug_forward:
+            print(f"   ðŸ“Š Mel spectrogram shape: {mel_spectrogram.shape}")
+            print(f"   ðŸ“Š Mel spectrogram range: [{mel_spectrogram.min().item():.4f}, {mel_spectrogram.max().item():.4f}]")
+        
+        return mel_spectrogram
 
-        return audio_frontend(waveform)
\ No newline at end of file
+    def set_debug_mode(self, debug_forward=False):
+        """
+        Ð’ÐºÐ»ÑŽÑ‡Ð°ÐµÑ‚ Ñ€ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð´Ð»Ñ Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ forward pass.
+        
+        Args:
+            debug_forward (bool): Ð»Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ forward pass
+        """
+        self._debug_forward = debug_forward
+        if debug_forward:
+            print(f"ðŸ› Ð ÐµÐ¶Ð¸Ð¼ Ð¾Ñ‚Ð»Ð°Ð´ÐºÐ¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½ Ð´Ð»Ñ {self.__class__.__name__}")
+            print(f"   ðŸŽµ Debug forward: {debug_forward}")
\ No newline at end of file
diff --git a/train.py b/train.py
index 90a7a4f..ea54b46 100644
--- a/train.py
+++ b/train.py
@@ -26,6 +26,7 @@ def main(config):
 
     project_config = OmegaConf.to_container(config)
     logger = setup_saving_and_logging(config)
+    
     writer = instantiate(config.writer, logger, project_config)
 
     if config.trainer.device == "auto":
@@ -34,20 +35,26 @@ def main(config):
         device = config.trainer.device
 
     # setup data_loader instances
-    # batch_transforms should be put on device
     dataloaders, batch_transforms = get_dataloaders(config, device)
 
     # build model architecture, then print to console
     model = instantiate(config.model).to(device)
+    
+    # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð² Ð¼Ð¾Ð´ÐµÐ»Ð¸
+    total_params = sum(p.numel() for p in model.parameters())
+    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
+    
     logger.info(model)
 
     # get function handles of loss and metrics
     loss_function = instantiate(config.loss_function).to(device)
+    
     metrics = instantiate(config.metrics)
 
     # build optimizer, learning rate scheduler
     trainable_params = filter(lambda p: p.requires_grad, model.parameters())
     optimizer = instantiate(config.optimizer, params=trainable_params)
+    
     lr_scheduler = instantiate(config.lr_scheduler, optimizer=optimizer)
 
     # epoch_len = number of iterations for iteration-based training
